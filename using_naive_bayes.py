# -*- coding: utf-8 -*-
"""using_naive_bayes.ipynb


Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H6euzNl0Y_KIYfmrNQwpLQH9WYuMn05l
"""

import string
from collections import Counter
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import nltk
from nltk import pos_tag
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

train_data = pd.read_csv("text_emotion.csv")

traindata=train_data.drop(['tweet_id', 'author'], axis=1)

traindata.head(10)

def cleaning(content):
  lower_case = content.lower()
  lower_case.replace("."," ")
  cleaned_text = lower_case.translate(str.maketrans('.',' ',string.punctuation))
  tokenized_words = word_tokenize(cleaned_text,"english")
  final_words = [word for word in tokenized_words if word not in stopwords.words("english")]
  clean_words=[]
  for word in final_words:
    clean_words.append(cons_dupl(word))
  postagged_words=nltk.pos_tag(clean_words)
  lemma_words=[]
  for word,y in postagged_words:
      lemma_words.append(WordNetLemmatizer().lemmatize(word))
  lemma_words = ls(lemma_words)
  return lemma_words

def cons_dupl(word):
     s=''
     for char in word:
       if s=='' or char!=s[len(s)-1]:
         s=s+char
     return s
def ls(content):
  listtostr = ""
  for word in content:
    listtostr +=" "+word
  return listtostr

traindata['cleaned_text'] = traindata['content'].apply(cleaning)

from sklearn.linear_model import LinearRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,confusion_matrix,r2_score,accuracy_score

from sklearn.feature_extraction.text import CountVectorizer

x_features = traindata['cleaned_text']
y_labels = traindata['sentiment']

cv = CountVectorizer()
X = cv.fit_transform(x_features)

x_train,x_test,y_train,y_test=train_test_split(X,y_labels,test_size=0.3,random_state=42)

nv_model = MultinomialNB()
nv_model.fit(x_train,y_train)

nv_model.score(x_test,y_test)

predictvalue = nv_model.predict(x_test)

predictvalue

import numpy as np

def callmodel1(text):
   vect = cv.transform(text).toarray()
   emotion = nv_model.predict(vect)
   pred_proba =nv_model.predict_proba(vect)
   max=np.max(pred_proba)
   pred_percent_for_all = dict(zip(nv_model.classes_,pred_proba[0]*100))
   print(pred_percent_for_all)
   return emotion,max,pred_percent_for_all

text = input("text here")
in_text = []
in_text.append(text)
output=callmodel1(in_text)
emotion = output[0]
statement=output[2]
maxim = outp
ut[1]
x = list(statement.keys())
y = list(statement.values())
print("emotion: ")
print(emotion)
print("strength:")
print(maxim)
plt.bar(x,y)
plt.show()
